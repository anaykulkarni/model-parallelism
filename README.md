# model-parallelism
Benchmarking results for three different Mixture of Experts (MoE) architectures: Simple MoE, Tensor Parallel MoE, and Expert Parallel MoE
